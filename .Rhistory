?test_that
library(testthat)
library(smsPOMDP)
test_check("smsPOMDP")
?test_check
test_check("~/smsPOMDP/tests/")
test_check("~/smsPOMDP/
")
test_dir("~/smsPOMDP/")
test_dir("~/smsPOMDP/tests")
context("smsPOMDP")
?test_that
test_that("appl base functions generates a policy", {
model <- system.file("models/example.pomdp",  package = "sarsop")
f <- tempfile()
res <- pomdpsol(model, output = f, precision = 10)
expect_true(file.exists(f))
expect_lt(as.numeric(res["final_precision"]), 10)
expect_true(grepl("target precision reached", res["end_condition"]))
})
context('sarsop')
test_that("appl base functions generates a policy", {
model <- system.file("models/example.pomdp",  package = "sarsop")
f <- tempfile()
res <- pomdpsol(model, output = f, precision = 10)
expect_true(file.exists(f))
expect_lt(as.numeric(res["final_precision"]), 10)
expect_true(grepl("target precision reached", res["end_condition"]))
})
library(sarsop)
test_that("appl base functions generates a policy", {
model <- system.file("models/example.pomdp",  package = "sarsop")
f <- tempfile()
res <- pomdpsol(model, output = f, precision = 10)
expect_true(file.exists(f))
expect_lt(as.numeric(res["final_precision"]), 10)
expect_true(grepl("target precision reached", res["end_condition"]))
})
test_that("pomdpeval, polgraph and pomdpsim run without error", {
model <- system.file("models/example.pomdp",  package = "sarsop")
policy <- tempfile()
res <- pomdpsol(model, output = policy, precision = 10, stdout = FALSE)
evaluation <- pomdpeval(model, policy, stdout = FALSE, steps = 10)
graph <- polgraph(model, policy, stdout = FALSE)
simulations <- pomdpsim(model, policy, stdout = FALSE, steps = 10)
})
?Interp_policy
#values for Sumatran tigers
pen = 0.1
p0 = 1-pen
pem = 0.05816
pm = 1 - pem
V = 175.133
Cm = 18.784
Cs = 10.840
d0 = 0.01
d = 0.78193
#buiding the matrices of the problem
t = smsPOMDP::tr(p0, pm, d0, d, V, Cm, Cs) #transition matrix
o = smsPOMDP::obs(p0, pm, d0, d, V, Cm, Cs)#observation matrix
r = smsPOMDP::rew(p0, pm, d0, d, V, Cm, Cs) #reward matrix
state_prior = c(1,0) #initial belief state
log_dir = tempdir()
id <- digest::digest(match.call())
infile <- paste0(log_dir, "/", id, ".pomdpx")
outfile <- paste0(log_dir, "/", id, ".policyx")
stdout <- paste0(log_dir, "/", id, ".log")
sarsop::write_pomdpx(t, o, r, disc, state_prior, file = infile)
status <- sarsop::pomdpsol(infile, outfile, stdout = stdout)
policy <- sarsop::read_policyx(file = outfile)
output <- smsPOMDP::Interp_policy(state_prior,policy$vectors,policy$action)
#buiding the matrices of the problem
t = smsPOMDP::tr(p0, pm, d0, d, V, Cm, Cs) #transition matrix
o = smsPOMDP::obs(p0, pm, d0, d, V, Cm, Cs)#observation matrix
r = smsPOMDP::rew(p0, pm, d0, d, V, Cm, Cs) #reward matrix
state_prior = c(1,0) #initial belief state
log_dir = tempdir()
id <- digest::digest(match.call())
infile <- paste0(log_dir, "/", id, ".pomdpx")
outfile <- paste0(log_dir, "/", id, ".policyx")
stdout <- paste0(log_dir, "/", id, ".log")
sarsop::write_pomdpx(t, o, r, disc, state_prior, file = infile)
disc = 0.9
sarsop::write_pomdpx(t, o, r, disc, state_prior, file = infile)
status <- sarsop::pomdpsol(infile, outfile, stdout = stdout)
policy <- sarsop::read_policyx(file = outfile)
output <- smsPOMDP::Interp_policy(state_prior,policy$vectors,policy$action)
output
output
t
?test_that
expect_length(1, 1)
expect_length(1:10, 10)
## Not run:
expect_length(1:10, 1)
expect_equal(t[1,1,1]+t[1,2,1],1)
dim(t)
#is the matrix stochastic
expect_equal(t[1,1,1]+t[1,2,1],1)
expect_equal(t[2,1,1]+t[2,2,1],1)
expect_equal(t[1,1,2]+t[1,2,2],1)
expect_equal(t[2,1,2]+t[2,2,2],1)
test_dir("~/smsPOMDP/tests")
pen = 0.1 #local probability of extinction P(extinct/extant, survey or nothing)
p0 = 1-pen #local probability of persitance P(extant/extant, manage)
pem = 0.05816 #local probability of extinction if managed P(extinct/extant, manage)
pm = 1 - pem #local probability of persistance if managed P(extant/extant, manage)
d0 = 0.01 #local probability of detection P(present/extant, manage or nothing)
d = 0.78193 #local probability of detection if surveyed P(present/extant, survey)
V = 175.133 #Estimated economic value of the species ($/yr)
Cm = 18.784 #Estimated cost of managing ($/yr)
Cs = 10.840 #Estimated cost of surveying ($/yr)
#buiding the matrices of the problem
o = smsPOMDP::obs(p0, pm, d0, d, V, Cm, Cs)#observation matrix
o
t
trace(tr, edit=t)
#buiding the matrices of the problem
r = smsPOMDP::rew(p0, pm, d0, d, V, Cm, Cs) #reward matrix
r
output
length(output)
policy
state_prior = c(0.5,0.5) #initial belief state
log_dir = tempdir()
id <- digest::digest(match.call())
infile <- paste0(log_dir, "/", id, ".pomdpx")
outfile <- paste0(log_dir, "/", id, ".policyx")
stdout <- paste0(log_dir, "/", id, ".log")
sarsop::write_pomdpx(t, o, r, disc, state_prior, file = infile)
status <- sarsop::pomdpsol(infile, outfile, stdout = stdout)
policy <- sarsop::read_policyx(file = outfile)
output <- smsPOMDP::Interp_policy(state_prior,policy$vectors,policy$action)
expect_length(output, 2)#value and actions
output
policy
?compute_belief
#Initial belief state
state_prior = c(0.9,0.1) #extant : 0.9, extinct : 0.1
#previous actions and observations
ac = c('Manage','Survey','Stop')
ob = c('Not_seen','Not_seen', 'Seen')
compute_belief(p0, pm, d0, d, V, Cm, Cs, state_prior, ac, ob)
current = compute_belief(p0, pm, d0, d, V, Cm, Cs, state_prior, ac, ob)#current belief state
expect_length(current,2)
expect_equal(current[1]+current[2],1)
?expect_failure
a <- 9
expect_lt(a, 10)
## Not run:
expect_lt(11, 10)
## End(Not run)
a <- 11
expect_gt(a, 10)
## Not run:
expect_gt(9, 10)
expect_gt(current[1],0)
#is current a distribution over 2 states
expect_length(current,2)#belief state: 2 states, length of current is 2
expect_equal(current[1]+current[2],1)#current should be a probability distribution
expect_gt(current[1],0)
expect_gt(current[2],0)
expect_lt(current[1],1)
expect_lt(current[2],2)
#is current a distribution over 2 states
expect_length(current,2)#belief state: 2 states, length of current is 2
expect_equal(current[1]+current[2],1)#current should be a probability distribution
expect_gte(current[1],0)
expect_gte(current[2],0)
expect_lte(current[1],1)
expect_lte(current[2],2)
pen = 0.1 #local probability of extinction P(extinct/extant, survey or nothing)
p0 = 1-pen #local probability of persitance P(extant/extant, manage)
pem = 0.05816 #local probability of extinction if managed P(extinct/extant, manage)
pm = 1 - pem #local probability of persistance if managed P(extant/extant, manage)
d0 = 0.01 #local probability of detection P(present/extant, manage or nothing)
d = 0.78193 #local probability of detection if surveyed P(present/extant, survey)
V = 175.133 #Estimated economic value of the species ($/yr)
Cm = 18.784 #Estimated cost of managing ($/yr)
Cs = 10.840 #Estimated cost of surveying ($/yr)
#Initial belief state
state_prior = c(0.9,0.1) #extant : 0.9, extinct : 0.1
#test1: last observation is seen: current belief state is [1,0], the species is extant
#previous actions and observations
ac = c('Manage','Survey','Stop')
ob = c('Seen','Not_seen','Not_seen')
current = compute_belief(p0, pm, d0, d, V, Cm, Cs, state_prior, ac, ob)#current belief state
#is current a distribution over 2 states
expect_length(current,2)#belief state: 2 states, length of current is 2
expect_gte(current[1],0)
expect_gte(current[2],0)
expect_lte(current[1],1)
expect_lte(current[2],2)
expect_equal(current[1]+current[2],1)#current should be a probability distribution
expect_lt(current[1],1)# last observation is seen: current belief state is [1,0], the species is extant
?compute_belief
?tab_actions
install.packages("MDPtoolbox")
library(MDPtoolbox)
?`MDPtoolbox-package`
trace(mdp_check_square_stochastic, edit =T)
check_square_stochastic = function(X){
error_msg = T
s1 <- dim(X)[1]
s2 <- dim(X)[2]
if (s1 != s2) {
error_msg <-F
}
else if (max(abs(rowSums(X) - rep(1, s2))) > 10^(-12)) {
error_msg <-F
}
else if (length(which(X < 0)) > 0) {
error_msg <-F
}
return(error_msg)
}
stopifnot(check_square_stochastic(t[,,1]))
check_square_stochastic(t[,,1])
stopifnot(check_square_stochastic(t[,,1]))
stopifnot(check_square_stochastic(t[,,2]))
stopifnot(check_square_stochastic(t[,,3]))
?mdp_check
trace(mdp_check, edit =T)
trace(sarsop, edit =T)
trace(write_pomdpx, edit =T)
#' @export
tab_actions = function(transition, observation, reward, state_prior, disc = 0.95){
stopifnot(check_square_stochastic(t[,,1]))
stopifnot(check_square_stochastic(t[,,2]))
stopifnot(check_square_stochastic(t[,,3]))
log_dir = tempdir()
id <- digest::digest(match.call())
infile <- paste0(log_dir, "/", id, ".pomdpx")
outfile <- paste0(log_dir, "/", id, ".policyx")
stdout <- paste0(log_dir, "/", id, ".log")
sarsop::write_pomdpx(transition, observation, reward, disc, state_prior, file = infile)
status <- sarsop::pomdpsol(infile, outfile, stdout = stdout)
policy <- sarsop::read_policyx(file = outfile)
output <- smsPOMDP::Interp_policy(state_prior,policy$vectors,policy$action)
state_posterior = matrix(state_prior, ncol = 2)
optimal_action = output[[2]]
Tmax = 100
for (i in c(1:(Tmax))) {
a1 = optimal_action[i]
o1 = 2 #we treat the case when the species is not seen
s_p <- smsPOMDP::update_belief(state_posterior[i, ], transition, observation, o1,
a1)
state_posterior = rbind(state_posterior, s_p)
output <- Interp_policy(s_p,policy$vectors,policy$action)
optimal_action = c(optimal_action, output[[2]])
}
a = optimal_action[1]
act = a
years = numeric()
while(sum(years) < length(optimal_action)){
if (length(unique(optimal_action))==1){
i = length(optimal_action)
years = c(years, i)
break
} else {
i = min(which(optimal_action!= a))-1
optimal_action = optimal_action[-c(1:i)]
a = optimal_action[1]
act = c(act,a)
years = c(years, i)
}
}
tab = data.frame(action = act, years = years)
return(tab)
}
t
is_error_detected <- FALSE
error_msg <- ""
if (is.list(t)) {
s1 <- dim(t[[1]])[1]
s2 <- dim(t[[1]])[2]
a1 <- length(t)
}
else {
s1 <- dim(t)[1]
s2 <- dim(t)[2]
a1 <- dim(t)[3]
}
is_error_detected <- FALSE
error_msg <- ""
if (is.list(t)) {
s1 <- dim(t[[1]])[1]
s2 <- dim(t[[1]])[2]
a1 <- length(t)
} else {
s1 <- dim(t)[1]
s2 <- dim(t)[2]
a1 <- dim(t)[3]
}
s1
s2
a1
if (s1 < 1 | a1 < 1 | s1 != s2) {
error_msg <- "MDP Toolbox ERROR: The transition matrix must be on the form P(S,S,A) with S : number of states greater than 0 and A : number of action greater than 0"
is_error_detected <- T
}
inherits
o
dim(o)
