library(sarsop)
library(smsPOMDP)
library(MDPtoolbox)
library(data.table)
library(dplyr)
source('C:/Users/User/Dropbox/OEH AI tools for adaptive management and monitoring/POMDP files/sim_mdp_momdp_policy.R')
source('C:/Users/User/Dropbox/OEH AI tools for adaptive management and monitoring/POMDP files/Duff/sim_mdp_parameter_uncertainty.R')
source('C:/Users/User/Dropbox/OEH AI tools for adaptive management and monitoring/POMDP files/Duff/finite_horizon.R')
## simuations random MDPs ####
read_policyx2 = function(file){
xml <- xml2::read_xml(file)
xml_vectors <- xml2::xml_find_all(xml, "//Vector")
get_vector <- function(v) as.numeric(strsplit(as.character(xml2::xml_contents(v)),
" ")[[1]])
get_action <- function(v) as.numeric(xml2::xml_attr(v, "action"))
get_obs <- function(v) as.numeric(xml2::xml_attr(v, 'obsValue'))
n_states <- length(get_vector(xml_vectors[[1]]))
alpha <- vapply(xml_vectors, get_vector, numeric(n_states))
alpha_action <- vapply(xml_vectors, get_action, double(1)) +
1
alpha_obs <- vapply(xml_vectors, get_obs, double(1)) +
1
list(vectors = alpha, action = alpha_action, obs = alpha_obs)
} #read momdp
Interp_policy2 = function (initial, obs, alpha, alpha_action, alpha_obs){
id = which(alpha_obs == obs)
alpha2 = alpha[,id]
alpha_action2 = alpha_action[id]
a <- initial %*% alpha2
if (sum(a == 0) == length(a)) {
output = list(0, 1)
}
else {
output = list(max(a), alpha_action2[which.max(a)])
}
output
}
mdp_compute_value = function(P, PR, discount, max_iter){
# computes the optimal value function using the value iteration algo
iter <- 0
V <- c(0,0)
is_done <- F
epsilon = 0.0001
thresh <- epsilon * (1 - discount)/discount
#max_iter = 50000
while (!is_done) {
iter <- iter + 1
Vprev <- V
bellman <- mdp_bellman_operator(P, PR, discount,
V)
V <- bellman[[1]]
policy <- bellman[[2]]
variation <- max(V - Vprev)
if (variation < thresh) {
is_done <- T
#print("MDP Toolbox: iterations stopped, epsilon-optimal policy found")
}
else if (iter == max_iter) {
is_done <- T
#print("MDP Toolbox: iterations stopped by maximum number of iteration condition")
}
}
return(list('V' = V, policy = policy))
}
mdp_bellman_operator_min = function (P, PR, discount, Vprev){
if (discount <= 0 | discount > 1) {
print("--------------------------------------------------------")
print("MDP Toolbox ERROR: Discount rate must be in ]0; 1]")
print("--------------------------------------------------------")
}
else if (is.list(P) & ifelse(!missing(Vprev), ifelse(length(Vprev) !=
dim(P[[1]])[1], T, F), F)) {
print("--------------------------------------------------------")
print("MDP Toolbox ERROR: Vprev must have the same dimension as P")
print("--------------------------------------------------------")
}
else if (!is.list(P) & ifelse(!missing(Vprev), ifelse(length(Vprev) !=
dim(P)[1], T, F), F)) {
print("--------------------------------------------------------")
print("MDP Toolbox ERROR: Vprev must have the same dimension as P")
print("--------------------------------------------------------")
}
else {
Q <- matrix(0, dim(PR)[1], dim(PR)[2])
if (is.list(P)) {
A <- length(P)
for (a in 1:A) {
Q[, a] <- as.matrix(PR[, a] + discount * P[a][[1]] %*%
Vprev)
}
}
else {
A <- dim(P)[3]
for (a in 1:A) {
Q[, a] <- PR[, a] + discount * P[, , a] %*%
Vprev
}
}
return(list(V = apply(Q, 1, min), policy = apply(Q,
1, which.min)))
}
}
mdp_compute_value_min = function(P, PR, discount, max_iter){
# computes the optimal value function using the value iteration algo
iter <- 0
V <- c(0,0)
is_done <- F
epsilon = 0.0001
thresh <- epsilon * (1 - discount)/discount
#max_iter = 50000
while (!is_done) {
iter <- iter + 1
Vprev <- V
bellman <- mdp_bellman_operator_min(P, PR, discount,
V)
V <- bellman[[1]]
policy <- bellman[[2]]
variation <- max(V - Vprev)
if (variation < thresh) {
is_done <- T
#print("MDP Toolbox: iterations stopped, epsilon-optimal policy found")
}
else if (iter == max_iter) {
is_done <- T
#print("MDP Toolbox: iterations stopped by maximum number of iteration condition")
}
}
return(list('V' = V, policy = policy))
}
##algorithm####
## MOMDP #9 ####
i=9
alpha_momdp = read_policyx2('C:/Users/User/Dropbox/OEH AI tools for adaptive management and monitoring/POMDP files/2 parameters/delta baseline policy 0.01/iadine_easy_delta_bs_9.policyx')
S = nrow(alpha_momdp$vectors)
alpha_momdp$obs
unique(alpha_momdp$obs)
dim(alpha_momdp$obs)
length(alpha_momdp$obs)
alpha2 <- alpha_momdp$vectors[, which(alpha_momdp$obs == 1)]
alpha2
dim(alpha2)
real_state = 1 #low
belief = matrix(rep(c(1/S,0), S), ncol = 2*S)
belief
belief
disc = 0.9#discount factor
delta = c(0.25, 0.1,0.05) # |p(high|Low,a1)- p(high|Low,a2)|
baseline = c(0.2,0.5,0.7) #p(high|Low)
model = expand.grid(delta, baseline)
d = model[i,1]
b = model[i,2]
#4 possibilities for matrices
#hidden models
m1 = matrix(c(b ,1-b,b ,1-b), nrow = 2, byrow = T)
m2 = matrix(c(b+d ,1-(b+d),b ,1-b), nrow = 2, byrow = T)
m3 = matrix(c(b ,1-b,b+d,1-(b+d)), nrow = 2, byrow = T)
m4 = matrix(c(b+d ,1-(b+d),b+d ,1-(b+d)), nrow = 2, byrow = T)
tr_mod1 = array(c(m4, m1), dim = c(2,2,2))#(F11a, F22a)
tr_mod2 = array(c(m2, m3), dim = c(2,2,2))#(F11a, F22b)
tr_mod3 = array(c(m3, m2), dim = c(2,2,2))#(F11b, F22a)
tr_mod4 = array(c(m1, m4), dim = c(2,2,2))#(F11b, F22b)
tr1 = matrix(0, ncol = 8, nrow = 8)
tr1[1:2, 1:2] = tr_mod1[,,1]
tr1[3:4, 3:4] = tr_mod2[,,1]
tr1[5:6, 5:6] = tr_mod3[,,1]
tr1[7:8, 7:8] = tr_mod4[,,1]
tr2 = matrix(0, ncol = 8, nrow = 8)
tr2[1:2, 1:2] = tr_mod1[,,2]
tr2[3:4, 3:4] = tr_mod2[,,2]
tr2[5:6, 5:6] = tr_mod3[,,2]
tr2[7:8, 7:8] = tr_mod4[,,2]
tr_momdp = array(c(tr1, tr2), dim = c(8,8,2))
tr_momdp
belief_mod =sum_2_by_2(belief)
belief_mod
mean(c(16.5, 18.5,12,11.5,16.5,10, 13.5, 15.6, 9.5))
mean(c(16.5, 18.5,12,11.5,16.5,10, 13.5, 15.6, 9.5, 15, 15))
mean(c(16.5, 18.5,12,11.5,16.5,10, 13.5, 15.6, 9.5, 15, 15))
mean(c(16.5, 18.5,12,11.5,16.5,10, 13.5, 15.6, 10, 15, 15))
?
sim
library(smsPOMDP)
?sim
library(designmatch)
?meantab
?sim
pen <- 0.1
p0 <- 1-pen
pem <- 0.05816
pm <- 1 - pem
V <- 175.133
Cm <- 18.784
Cs <- 10.840
d0 <- 0.01
dm <- 0.01
ds <- 0.78193
#Initial belief state
state_prior <- c(0.9,0.1) #extant : 0.9, extinct : 0.1
#Horizon of the simulation
Tmax <- 20
simulations_tab(p0, pm, d0, dm, ds, V, Cm, Cs, state_prior, Tmax)
#' @export
simulations_tab<-function(p0, pm, d0, dm, ds, V, Cm, Cs, state_prior, Tmax,
disc = 0.95, size = 1, nbSimul = 100) {
stopifnot(p0 >= 0, p0 <= 1)
stopifnot(pm >= 0, pm <= 1)
stopifnot(d0 >= 0, d0 <= 1)
stopifnot(dm >= 0, dm <= 1)
stopifnot(ds >= 0, ds <= 1)
stopifnot(V >= 0, Cm >= 0, Cs >= 0)
stopifnot(sum(state_prior) == 1)
stopifnot(state_prior >= 0, state_prior <= 1)
stopifnot(Tmax >= 0)
stopifnot(disc >= 0, disc <= 1)
t <- smsPOMDP::tr(p0, pm, d0, dm, ds, V, Cm, Cs)
o <- smsPOMDP::obs(p0, pm, d0, dm, ds, V, Cm, Cs)
r <- smsPOMDP::rew(p0, pm, d0, dm, ds, V, Cm, Cs)
stopifnot(smsPOMDP::check_pomdp(t, o, r))
alpha <- sarsop::sarsop(t, o, r, disc, state_prior)
log_dir <- tempdir()
id <- digest::digest(match.call())
infile <- paste0(log_dir, "/", id, ".pomdpx")
outfile <- paste0(log_dir, "/", id, ".policyx")
stdout <- paste0(log_dir, "/", id, ".log")
sarsop::write_pomdpx(t, o, r, disc, state_prior, file = infile)
status <- sarsop::pomdpsol(infile, outfile, stdout = stdout)
policy <- sarsop::read_policyx(file = outfile)
for (simu in seq_len(nbSimul)){
set.seed(as.integer((as.double(Sys.time()) * 1000 + Sys.getpid())%%2^31))
rand <- stats::runif(1)
if (rand <= state_prior[1]) {
real_state <- 1
}
else {
real_state <- 2
}
output <- smsPOMDP::interp_policy(state_prior, policy$vectors,
policy$action)
actions <- output[[2]][1]
set.seed(as.integer((as.double(Sys.time()) * 2000 + Sys.getpid())%%2^31))
rand <- stats::runif(1)
if (rand <= o[real_state, 1, actions]) {
observations <- 1
}
else {
observations <- 2
}
state_posterior <- matrix(state_prior, ncol = 2)
rewards <- r[real_state, actions]
for (i in seq_len(Tmax)) {
a1 <- actions[i]
o1 <- observations[i]
s_p <- smsPOMDP::update_belief(state_posterior[i, ],
t, o, o1, a1)
s_p <- s_p/sum(s_p)
state_posterior <- rbind(state_posterior, s_p)
set.seed(as.integer((as.double(Sys.time()) * i * 1000 +
Sys.getpid())%%2^31))
rand <- stats::runif(1)
if (rand < t[real_state[i], 1, a1]) {
r_s <- 1
real_state <- c(real_state, 1)
}
else {
r_s <- 2
real_state <- c(real_state, 2)
}
output <- smsPOMDP::interp_policy(s_p, policy$vectors,
policy$action)
actions <- c(actions, output[[2]][1])
rewards <- c(rewards, disc**i*r[r_s, output[[2]][1]])
set.seed(as.integer((as.double(Sys.time()) * i * 21000 +
Sys.getpid())%%2^31))
rand <- stats::runif(1)
if (rand <= o[r_s, 1, actions[i + 1]]) {
observations <- c(observations, 1)
}
else {
observations <- c(observations, 2)
}
}
if (simu == 1){
datasimu <- state_posterior[,1]
rewardsim <- matrix(rewards, ncol = 1)
} else {
datasimu <- cbind(datasimu, state_posterior[,1])
rewardsim <- cbind(rewardsim, rewards)
}
}
mean_belief <- apply(datasimu, 1, mean)
sd_belief <- apply(datasimu, 1, sd)
up_belief <- pmin(mean_belief + 1.96 * sd_belief, rep(1, length(mean_belief)))
low_belief <- pmax(mean_belief - 1.96 * sd_belief, rep(0, length(mean_belief)))
mean_reward <- apply(rewardsim, 1, mean)
sd_reward <-  apply(rewardsim, 1, sd)
up_reward <- mean_reward + 1.96 * sd_reward
low_reward <- mean_reward - 1.96 * sd_reward
data = data.frame(mean_belief=mean_belief,
up_belief = up_belief,
low_belief = low_belief,
mean_reward = mean_reward,
up_reward = up_reward,
low_reward = low_reward
)
return(data)
}
pen <- 0.1
p0 <- 1-pen
pem <- 0.05816
pm <- 1 - pem
V <- 175.133
Cm <- 18.784
Cs <- 10.840
d0 <- 0.01
dm <- 0.01
ds <- 0.78193
#Initial belief state
state_prior <- c(0.9,0.1) #extant : 0.9, extinct : 0.1
#Horizon of the simulation
Tmax <- 20
simulations_tab(p0, pm, d0, dm, ds, V, Cm, Cs, state_prior, Tmax)
pen <- 0.1
p0 <- 1-pen
pem <- 0.05816
pm <- 1 - pem
V <- 175.133
Cm <- 18.784
Cs <- 10.840
d0 <- 0.01
dm <- 0.01
ds <- 0.78193
#Initial belief state
state_prior <- c(0.9,0.1) #extant : 0.9, extinct : 0.1
#Horizon of the simulation
Tmax <- 20
data <- simulations_tab(p0, pm, d0, dm, ds, V, Cm, Cs, state_prior, Tmax)
data
library(testthat)
setwd("~/smsPOMDP/")
>usethis::use_testthat
?usethis::use_testthat
?usethis::use_test
usethis::use_test()
?sim
pen <- 0.1
p0 <- 1-pen
pem <- 0.05816
pm <- 1 - pem
V <- 175.133
Cm <- 18.784
Cs <- 10.840
d0 <- 0.01
dm <- 0.01
ds <- 0.78193
#Initial belief state
state_prior <- c(1,0)
Tmax <-20
data <- simulations_tab(p0, pm, d0, dm, ds, V, Cm, Cs, state_prior, Tmax)
data
testthat::expect_named(data)
testthat::expect_named(data, c("mean_belief","up_belief","low_belief","mean_reward","up_reward","low_rewar"))
testthat::expect_named(data, c("mean_belief","up_belief","low_belief","mean_reward","up_reward","low_reward"))
#testing data output
testthat::expect_equal(nrow(data), Tmax+1)
testthat::expect_equal(ncol(data), 6)
testthat::expect_named(data, c("mean_belief","up_belief","low_belief","mean_reward","up_reward","low_reward"))
head(data)
library(testthat)
setwd("~/smsPOMDP/")
#
document()
#
devtools::document()
usethis::use_test()#in the current file
#' @export
reward_belief <- function(p0, pm, d0, dm, ds, V, Cm, Cs,
state_posterior, act, disc = 0.95, size = 1){
stopifnot(p0 >= 0, p0 <= 1)
stopifnot(pm >= 0, pm <= 1)
stopifnot(d0 >= 0, d0 <= 1)
stopifnot(dm >= 0, dm <= 1)
stopifnot(ds >= 0, ds <= 1)
stopifnot(V >= 0, Cm >= 0, Cs >= 0)
stopifnot(disc >= 0, disc <= 1)
r <- smsPOMDP::rew(p0, pm, d0, dm, ds, V, Cm, Cs)
conv_action <- function(asdf) {
switch(asdf, Manage = 1, Survey = 2, Stop = 3)
}
action <- unlist(lapply(act, conv_action))
rewards <- c()
for (i in seq(1, min(nrow(state_posterior), length(action)))){
rew <- state_posterior[i,1]*r[1, action[i]] + state_posterior[i,2]*r[2, action[i]]
rewards <- c(rewards, rew)
}
return(rewards)
}
act <- c("Manage", "Survey", "Stop")
obs <- c("Seen", "Seen", "Not seen")
state_prior <- c(1,0)
state_posterior<-compute_belief2(p0, pm, d0, dm, ds, V, Cm, Cs, state_prior, act, obs)
compute_belief2 <- function (p0, pm, d0, dm, ds, V, Cm, Cs, state_prior, act, obs,
disc = 0.95, size = 1) {
stopifnot(p0 >= 0, p0 <= 1)
stopifnot(pm >= 0, pm <= 1)
stopifnot(d0 >= 0, d0 <= 1)
stopifnot(dm >= 0, dm <= 1)
stopifnot(ds >= 0, ds <= 1)
stopifnot(V >= 0, Cm >= 0, Cs >= 0)
stopifnot(disc >= 0, disc <= 1)
stopifnot(length(act) == length(obs))
t <- smsPOMDP::tr(p0, pm, d0, dm, ds, V, Cm, Cs)
o <- smsPOMDP::obs(p0, pm, d0, dm, ds, V, Cm, Cs)
r <- smsPOMDP::rew(p0, pm, d0, dm, ds, V, Cm, Cs)
conv_action <- function(asdf) {
switch(asdf, Manage = 1, Survey = 2, Stop = 3)
}
action <- unlist(lapply(act, conv_action))
conv_obs <- function(asdf) {
if (asdf=="Seen"){
return(1)
} else if (asdf=="Not seen"){
return(2)
}
}
observation <- unlist(lapply(obs, conv_obs))
state_posterior <- matrix(state_prior, ncol = 2)
for (i in seq_len(length(act))) {
a1 <- action[i]
o1 <- observation[i]
s_p <- smsPOMDP::update_belief(state_posterior[i,
], t, o, o1, a1)
s_p <- s_p/sum(s_p)
state_posterior <- rbind(state_posterior, s_p)
}
b <- state_posterior[nrow(state_posterior), ]
# b <- b/sum(b)
# return(b)
return(state_posterior)
}
reward_belief <- function(p0, pm, d0, dm, ds, V, Cm, Cs,
state_posterior, act, disc = 0.95, size = 1){
stopifnot(p0 >= 0, p0 <= 1)
stopifnot(pm >= 0, pm <= 1)
stopifnot(d0 >= 0, d0 <= 1)
stopifnot(dm >= 0, dm <= 1)
stopifnot(ds >= 0, ds <= 1)
stopifnot(V >= 0, Cm >= 0, Cs >= 0)
stopifnot(disc >= 0, disc <= 1)
r <- smsPOMDP::rew(p0, pm, d0, dm, ds, V, Cm, Cs)
conv_action <- function(asdf) {
switch(asdf, Manage = 1, Survey = 2, Stop = 3)
}
action <- unlist(lapply(act, conv_action))
rewards <- c()
for (i in seq(1, min(nrow(state_posterior), length(action)))){
rew <- state_posterior[i,1]*r[1, action[i]] + state_posterior[i,2]*r[2, action[i]]
rewards <- c(rewards, rew)
}
return(rewards)
}
state_posterior<-compute_belief2(p0, pm, d0, dm, ds, V, Cm, Cs, state_prior, act, obs)
state_posterior
reward_belief
state_posterior
reward_belief(p0, pm, d0, dm, ds, V, Cm, Cs,state_posterior, act)
rew_tab <- reward_belief(p0, pm, d0, dm, ds, V, Cm, Cs,state_posterior, act)
rew_tab
testthat::expect_equal(length(rew_tab), length(act))
state_posterior
rew_tab[1]
V-Cm
testthat::expect_equal(rew_tab[1],V-Cm)
testthat::expect_equal(unname(rew_tab[1]), V-Cm)
testthat::expect_equal(unname(rew_tab[2]), V-Cs)
testthat::expect_equal(unname(rew_tab[3]), V)
rew_tab
?tab_actions
library(smsPOMDP)
?tab_actions
?smsPOMDP::past_actions
?compute_belief
usethis::use_test()#in the current file
state_posterior
testthat::expect_equal(nrow(state_posterior), length(act)+1)
testthat::expect_equal(ncol(state_posterior), 2)
testhat::expect_equal(state_prior[,1]+state_prior[,2], 1)
testthat::expect_equal(state_prior[,1]+state_prior[,2], 1)
state_prior[,1]+state_prior[,2]
testthat::expect_equal(state_posterior[,1]+state_posterior[,2], 1)
(state_posterior[,1]+state_posterior[,2])
#testing on Sumatran tiger values
pen <- 0.1
p0 <- 1-pen
pem <- 0.05816
pm <- 1 - pem
V <- 175.133
Cm <- 18.784
Cs <- 10.840
d0 <- 0.01
dm <- 0.01
ds <- 0.78193
act <- c("Manage", "Survey", "Stop")
obs <- c("Seen", "Seen", "Not seen")
state_prior <- c(1,0)
state_posterior<-smsPOMDP::compute_belief_list(p0, pm, d0, dm, ds, V, Cm,
Cs, state_prior, act, obs)
testthat::expect_equal(nrow(state_posterior), length(act)+1)
testthat::expect_equal(ncol(state_posterior), 2)
testthat::expect_equal(unname(state_posterior[,1]+state_posterior[,2]),
rep(1,length(act)+1))
#
devtools::document()
